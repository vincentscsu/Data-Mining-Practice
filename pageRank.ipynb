{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PageRank to Rank the Importance of Web Pages\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize the web graph\n",
    "---\n",
    "In this step we initialize a web graph and count the out-links of each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "web = [[0,0],[0,1],[1,0],[1,2]] # Each pair stands for a connection from the left member to\n",
    "                                # the right. e.g the second pair means 0 had an out-link towards 1.\n",
    "n = 3\n",
    "# get out-links\n",
    "outLinks = {}\n",
    "for each in web:\n",
    "    if each[0] in outLinks:\n",
    "        outLinks[each[0]] += 1\n",
    "    else:\n",
    "        outLinks[each[0]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Form a column adjacency matrix\n",
    "---\n",
    "This marix contains information about outlinks and inlinks of each node. We will use this matrix to compute our page rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense matrix =\n",
      "[[ 0.5  0.5  0. ]\n",
      " [ 0.5  0.   0. ]\n",
      " [ 0.   0.5  0. ]]\n"
     ]
    }
   ],
   "source": [
    "# form column adjacency matrix\n",
    "import numpy as np\n",
    "\n",
    "M = np.zeros((n,n))\n",
    "for each in web:\n",
    "    i = each[0]\n",
    "    j = each[1]\n",
    "    M[j][i] = 1. / outLinks[i]\n",
    "\n",
    "print 'Dense matrix =' \n",
    "print M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transform our matrix to a sparse matrix\n",
    "---\n",
    "An adjacency matrix is usually very sparse, so if we use the dense matrix format it will wastes a lot of space, so we use scipy to turn it into a sparse matrix, in preparation for future use on large web graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix =\n",
      "  (0, 0)\t0.5\n",
      "  (0, 1)\t0.5\n",
      "  (1, 0)\t0.5\n",
      "  (2, 1)\t0.5\n"
     ]
    }
   ],
   "source": [
    "# transform M into a sparse matrix\n",
    "from scipy import *\n",
    "from scipy.sparse import *\n",
    "\n",
    "S = csr_matrix(M)\n",
    "print 'Sparse matrix ='\n",
    "print S "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate PageRank\n",
    "---\n",
    "Now we can use the matrix to compute our PageRank. We first initialize our rank vector as an array of $\\frac{1}{n}$, and we repeatedly compute the PageRank until it converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank = [  2.08940652e-08   1.29132425e-08   7.98082275e-09]\n",
      "Iteration = 79\n"
     ]
    }
   ],
   "source": [
    "# create rank vector r (r sums to 1)\n",
    "r = np.array([1./n] * n)\n",
    "\n",
    "# compute page rank\n",
    "epsilon = 1.0e-8\n",
    "delta = 1\n",
    "iter = 0\n",
    "while delta > epsilon:\n",
    "    rNew = S.dot(r)\n",
    "    delta = sum(map(abs,rNew - r))\n",
    "    r = rNew\n",
    "    iter += 1\n",
    "    \n",
    "print 'PageRank =',r\n",
    "print 'Iteration =', iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Let's make it a function\n",
    "----\n",
    "Looks like our algorithm works! Since there are more inlinks going into node 0 and node 1, they must have higher rank than node 2. And most links go to node 0, so it's natural that it has the highest rank. Our result seems to agree with this observation. So let's package our algorithm into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computePageRank(M, r, epsilon=1.0e-8):\n",
    "    \"\"\"\n",
    "    Function to compute page rank with a given adjacency matrix and rank vector\n",
    "    \"\"\"\n",
    "    delta = 1\n",
    "    iter = 0\n",
    "    while delta > epsilon:\n",
    "        rNew = M.dot(r)\n",
    "        delta = sum(map(abs,rNew - r))\n",
    "        r = rNew\n",
    "        iter += 1\n",
    "    print 'PageRank =',r\n",
    "    print 'Iteration =', iter\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Spider web and dead-end problem\n",
    "---\n",
    "As discussed in the article, there are two possible problems when calculating PageRank, the spider web scenario where most of the information is 'absorbed' in a subset of the graph, and the dead-end problem where information leaks out because there is no out-link at dead-end.\n",
    "\n",
    "In our scenario, we have a spider web between 0 and 1, and a dead end at 2. No wonder our page rank becomes so small after the computation, because information leaked out!\n",
    "\n",
    "To account for this problem, we construct a stochastic matrix where each column sums to one, by filling in $\\frac{1}{n}$ for each column that's all 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New M matrix:\n",
      "[[ 0.5         0.5         0.33333333]\n",
      " [ 0.5         0.          0.33333333]\n",
      " [ 0.          0.5         0.33333333]]\n",
      "\n",
      "PageRank = [ 0.46153846  0.30769231  0.23076923]\n",
      "Iteration = 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.46153846,  0.30769231,  0.23076923])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a stochastic matrix (columns sum to 1) to account for dead end\n",
    "\n",
    "MNew = np.copy(M)\n",
    "for i in range(3):\n",
    "    MNew[i][2] += 1./3\n",
    "print 'New M matrix:'\n",
    "print MNew\n",
    "print\n",
    "\n",
    "# compute page rank\n",
    "r = np.array([1./n] * n)\n",
    "computePageRank(MNew, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use Google matrix and teleport parameter $\\beta$ \n",
    "---\n",
    "Now our PageRank looks a lot better, and they sum to 1, which is the sum of originally assigned $\\frac{1}{n}$ for each node. Let's try to use google matrix and the teleportation method to solve the problem and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My google matrix:\n",
      "[[ 0.46666667  0.46666667  0.06666667]\n",
      " [ 0.46666667  0.06666667  0.06666667]\n",
      " [ 0.06666667  0.46666667  0.06666667]]\n",
      "\n",
      "PageRank = [  1.76227715e-08   1.18132599e-08   8.95626491e-09]\n",
      "Iteration = 82\n",
      "\n",
      "PageRank * 1.0e8 = [ 1.76227715  1.18132599  0.89562649]\n"
     ]
    }
   ],
   "source": [
    "# use google matrix and teleport parameter beta\n",
    "beta = 0.8\n",
    "e = ones((n,n))\n",
    "\n",
    "A = M.dot(beta) + (1-beta) * 1./n * e\n",
    "print 'My google matrix:'\n",
    "print A\n",
    "print \n",
    "\n",
    "r = np.array([1./n] * n)\n",
    "res = computePageRank(A, r)\n",
    "\n",
    "print\n",
    "print 'PageRank * 1.0e8 =',res*1.0e8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the PageRank becomes very small again, though the ratio between our computed vectors remain close to each other. I multiply it by $10^8$ just to see the ratio better.\n",
    "\n",
    "Now we can construct a sparse matrix using this $\\beta$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank = [ 0.21212122  0.15151516  0.12727273]\n"
     ]
    }
   ],
   "source": [
    "# use sparse matrix with teleport parameter\n",
    "\n",
    "r = np.array([1./n] * n)\n",
    "epsilon = 1.0e-8\n",
    "delta = 1\n",
    "one = ones(n)\n",
    "while delta > epsilon:\n",
    "    rNew = S.dot(beta).dot(r) + (1-beta) / n * one\n",
    "    delta = sum(map(abs,rNew - r))\n",
    "    r = rNew\n",
    "\n",
    "print 'PageRank =' ,r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again package our algorithm into a function for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pageRank(web, n, beta=0.8, epsilon = 1.0e-8):\n",
    "    \"\"\"\n",
    "    Compute the pagerank of n nodes in a given directed web graph\n",
    "    \"\"\"\n",
    "    # get out-links\n",
    "    outLinks = {}\n",
    "    for each in web:\n",
    "        if each[0] in outLinks:\n",
    "            outLinks[each[0]] += 1\n",
    "        else:\n",
    "            outLinks[each[0]] = 1\n",
    "\n",
    "    # form base matrix \n",
    "    M = np.zeros((n,n))\n",
    "    for each in web:\n",
    "        i = each[0]\n",
    "        j = each[1]\n",
    "        M[j][i] = 1. / outLinks[i]\n",
    "    \n",
    "    # form sparse matrix\n",
    "    S = csr_matrix(M)\n",
    "    \n",
    "    # create rank vector r (r sums to 1)\n",
    "    r = np.array([1./n] * n)\n",
    "\n",
    "    # compute page rank     \n",
    "    delta = 1\n",
    "    one = ones(n)\n",
    "    while delta > epsilon:\n",
    "        rNew = S.dot(beta).dot(r) + (1-beta) / n * one\n",
    "        delta = sum(map(abs,rNew - r))\n",
    "        r = rNew\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Tests\n",
    "---\n",
    "Now we can test our PageRank algorithm on our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21212122,  0.15151516,  0.12727273])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test scalable page rank algorithm on small case\n",
    "\n",
    "pageRank(web, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 11342] [916425, 837379]\n"
     ]
    }
   ],
   "source": [
    "# deploy on large graph\n",
    "\n",
    "# read the file\n",
    "\n",
    "myFile = open('web-Google.txt', 'r')\n",
    "\n",
    "# skip the first few description lines\n",
    "for _ in range(4):\n",
    "    myFile.readline()\n",
    "\n",
    "# start reading each line\n",
    "largeWeb = []\n",
    "t = myFile.readline()\n",
    "while t:\n",
    "    largeWeb.append([int(x) for x in (t.split())]) # strip off unrelated characters such as '\\t', '\\r', etc.\n",
    "    t = myFile.readline()\n",
    "    \n",
    "# verify I have read all lines properly\n",
    "print largeWeb[0], largeWeb[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916428\n"
     ]
    }
   ],
   "source": [
    "# find n\n",
    "n = 0\n",
    "for i in xrange(len(largeWeb)):\n",
    "    current = max(largeWeb[i][0], largeWeb[i][1])\n",
    "    if current > n:\n",
    "        n = current\n",
    "# number of nodes is the maximum number plus 1, accounting for node 0\n",
    "n += 1\n",
    "print n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute page rank for large web graph\n",
    "### MemoryError at this step ###\n",
    "\n",
    "rank = pageRank(largeWeb, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Improve algorithm for large scale PageRank\n",
    "---\n",
    "Now the next step is to optimize the algorithm to handle memory efficiently while computing the correct PageRake."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
